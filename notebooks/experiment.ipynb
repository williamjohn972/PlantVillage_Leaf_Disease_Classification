{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d128b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the project root directory\n",
    "project_root = os.path.abspath(\"..\")   # one level above /notebooks\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178a006",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a143c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "from src.train_utils import build_model, build_trainer, build_optim, calc_class_weights\n",
    "from src.dataset import get_datasets, create_data_loaders "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb669d2",
   "metadata": {},
   "source": [
    "**Setting Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847d75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_set, val_set, test_set, classes = get_datasets(\"../data/PlantVillage\",val_split=0.1,test_split=0.1)\n",
    "num_classes = len(classes)\n",
    "\n",
    "class_weights = calc_class_weights(train_set, DEVICE) # These weights are to be passed into the loss function later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ebd5f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Class Weights ----------\n",
      "Class 0: 0.802\n",
      "Class 1: 0.541\n",
      "Class 2: 0.799\n",
      "Class 3: 0.799\n",
      "Class 4: 5.240\n",
      "Class 5: 0.376\n",
      "Class 6: 0.799\n",
      "Class 7: 0.419\n",
      "Class 8: 0.839\n",
      "Class 9: 0.451\n",
      "Class 10: 0.477\n",
      "Class 11: 0.569\n",
      "Class 12: 0.249\n",
      "Class 13: 2.138\n",
      "Class 14: 0.502\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'-'*10} Class Weights {'-'*10}\")\n",
    "for target, weight in enumerate(class_weights):\n",
    "\n",
    "    print(f\"Class {target}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689da441",
   "metadata": {},
   "source": [
    "# **HyperParam Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c19782",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "\n",
    "    # Model HyperParams  \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=16)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7, step=0.1)\n",
    "\n",
    "    \n",
    "    # Other Params\n",
    "    early_stopper_patience = 10\n",
    "    delta = 1e-4\n",
    "    lr_patience = early_stopper_patience // 2\n",
    "    lr_factor = 0.75\n",
    "    min_lr = 1e-6\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader, val_loader, _  = create_data_loaders(train_dataset=train_set,\n",
    "                                                       val_dataset=val_set,\n",
    "                                                       test_dataset=None,\n",
    "                                                       batch_size=batch_size)\n",
    "    # Model\n",
    "    model = build_model(num_classes=num_classes, dropout=dropout, freeze_base=True,\n",
    "                        device=DEVICE)\n",
    "    \n",
    "    # Loss, Optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights) \n",
    "    optim = build_optim(model=model, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "                                  \n",
    "    # Trainer\n",
    "    trainer = build_trainer(model, loss_fn, optim,\n",
    "                            early_stopper_patience, delta, False, None,\n",
    "                            lr_factor, lr_patience, min_lr,\n",
    "                            DEVICE)\n",
    "\n",
    "    # Train Model\n",
    "    trainer.train_val_model(epochs=EPOCHS,\n",
    "                            train_loader=train_loader,\n",
    "                            val_loader=val_loader)\n",
    "    \n",
    "    val_f1 = max(trainer.history['val_f1s'])\n",
    "\n",
    "    return val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9466237",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"plant_village_study\",\n",
    "    direction=\"maximize\",    # because we want to maximise the f1 score \n",
    "    storage=\"sqlite:///plant_village_study.db\",\n",
    "    load_if_exists=True)  \n",
    "\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best val f1-macro: {study.best_value}\")\n",
    "print(f\"Best hyperparameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbe483",
   "metadata": {},
   "source": [
    "# **ReTrain the Model Using Best HyperParam Combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(study_name=\"plant_village_study\", storage=\"sqlite:///plant_village_study.db\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa4257",
   "metadata": {},
   "source": [
    "### **Training Phase 1 - Frozen BackBone**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba948b4",
   "metadata": {},
   "source": [
    "### **Training Phase 2 - Unfrozen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Best Model\n",
    "# best_model_checkpoint = \n",
    "# new_best_model_checkpoint = \n",
    "\n",
    "# model = build_model(num_classes=num_classes, dropout=, freeze_base=False)\n",
    "# model.load_state_dict(torch.load(best_model_checkpoint['model_state_dict']))\n",
    "\n",
    "\n",
    "# # Lower lr\n",
    "# lr = \n",
    "\n",
    "\n",
    "# # Data Loaders\n",
    "# train_loader, val_loader, _  = create_data_loaders(train_dataset=train_set,\n",
    "#                                                     val_dataset=val_set,\n",
    "#                                                     test_dataset=None,\n",
    "#                                                     batch_size=batch_size)\n",
    "\n",
    "\n",
    "# # Loss, Optimizer\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optim = build_optim(model=model, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "                               \n",
    "# # Trainer\n",
    "# trainer = build_trainer(model, loss_fn, optim,\n",
    "#                         early_stopper_patience, delta, checkpoint_path,\n",
    "#                         lr_factor, lr_patience, min_lr,\n",
    "#                         DEVICE)\n",
    "\n",
    "# # Train Model\n",
    "# trainer.train_val_model(epochs=EPOCHS,\n",
    "#                         train_loader=train_loader,\n",
    "#                         val_loader=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fecbf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Metric Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, test_loader  = create_data_loaders(train_dataset=None, val_dataset=None,\n",
    "#                                          test_dataset=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f362f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def are_dataloaders_identical(dl1, dl2):\n",
    "#     # Check if lengths are the same\n",
    "#     if len(dl1) != len(dl2):\n",
    "#         return False\n",
    "    \n",
    "#     # Iterate through both dataloaders simultaneously\n",
    "#     for batch1, batch2 in zip(dl1, dl2):\n",
    "#         # Assuming batches are tensors or tuples of tensors\n",
    "#         if isinstance(batch1, torch.Tensor):\n",
    "#             if not torch.equal(batch1, batch2):\n",
    "#                 return False\n",
    "#         elif isinstance(batch1, (list, tuple)):\n",
    "#             for t1, t2 in zip(batch1, batch2):\n",
    "#                 if not torch.equal(t1, t2):\n",
    "#                     return False\n",
    "#         else:\n",
    "#             # Handle other data types in batches as needed\n",
    "#             if batch1 != batch2:\n",
    "#                 return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4a851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf-disease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
